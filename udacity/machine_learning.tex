\documentclass{article}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{framed} %for important notes that need to be framed
\usepackage{tikz}
\usepackage{multicol} %for multi-column bullet points
\usepackage{float} %for placing figures at the exact place as in the code
\usepackage{lscape} %for inserting landscape pages

\usepackage[a4paper, total={7in, 10in}]{geometry} %for formatting the sheet size

\usepackage{graphicx}
\graphicspath{ {./visualizations/} } %for inserting figures

\usepackage{hyperref} %for hyperlinks
\hypersetup{
    colorlinks = true,
    linkcolor = blue,
    filecolor = magenta,
    urlcolor = cyan,
}

\title{Udacity Machine Learning Nanodegree - Course Notes}

\author{Ruslan Kozhuharov}

\begin{document}
\pagenumbering{gobble}
\maketitle
\tableofcontents
\newpage
\pagenumbering{arabic}
\setlength{\parskip}{1em}

\section{Model Evaluation}
\subsection{Confusion Matrix}
The following table is called a confusion matrix:

\begin{center}
\begin{tabular}{ |l|l|l| }
  \hline
   & predicted 1 & predicted 0 \\
  \hline
  actual 1 & true positive & false negative \\
  actual 0 & false positive & false negative \\
  \hline
\end{tabular}
\end{center}

The total number of cases is: true positives + true negatives + false negatives + false positives.

\subsection{Accuracy}
Accuracy is defined as: (true positives + true negatives) / total cases.
If the data is severely skewed, the accuracy metric is misleading. If 250 000 credit card transactions are non-fraudulent and there are 500 fraudulent, a model that always predicts a non-fraudulent transaction has an accuracy of 99.8%.

\subsection{Precision}
Precision is defined as: out of all points predicted to be 1 how many are actually 1? The formulation is: true positives / (true positives + false positives). This corresponds to the first column of the confusion matrix. A spam filter model needs to have a high precision. This is because we want to have as few false positives (emails mistakenly identified as spam) as possible.

\subsection{Recall}
Recall can be defined as: out of all points that actually are 1, how many are correctly predicted as 1? The formulation is: true positives / (true positives + false negatives). This corresponds to the first row of the confusion matrix. A medical model needs to have a high recall in order to catch as many sick (positives) patients as possible. This prioritizes labeling the 1s correctly. This is because if a patient is falsely diagnosed as sick, they could be sent for further tests and could have some discomfort but no sick patient will be sent home.

\subsection{F1 Score}
F1 score is defined as:

\begin{equation}
  F1 = 2 * \frac{precision * recall}{precision + recall}
\end{equation}

This is the harmonic mean of precision and recall.

\subsection{Fbeta Score}
Fbeta score is defined as:

\begin{equation}
  F_\beta = (1 + \beta^2) * \frac{precision * recall}{\beta^2 * precision + recall}
\end{equation}

Lower beta values skew the formula towards precision ($F_0 = precision$) and higher beta values skew the score towards recall ($F_\infty = recall$).

True positive rate is true positives / all positives, while the false positive rate is the false positives / all negatives.

\subsection{ROC Curves}
Receiver Operating Characteristic (ROC) curves. This is a curve created by plotting the true positive rate and false positive rate as the x, y coordinates of every possible split of the model. This gives us a 2 dimensional curve. The area under this curve is the ROC score. It is 1 for a perfect classifier and 0.5 for a random classifier. The ROC score could be 0 if the model misclassifies every single point.

\subsection{Regression Metrics}

\begin{itemize}
  \item mean absolute error
  \item mean squared error (MSE)
  \item R2 score
\end{itemize}

R2 score is a comparison between our model and the simplest possible model. For a linear regression, such a baseline model could just be a horizontal line drawn between the points. In this case, the R2 score is:

\begin{equation}
  R2 = 1 - \frac{MSE (current)}{MSE (baseline)}
\end{equation}

\section{Model Selection}
\subsection{K-Fold Cross Validation}
It’s method of recycling our data in order to better utilize it for training and testing. Using this method we don’t ‘lose’ data for testing without violating the rule of use of the test set (never use the test set for training).

\begin{enumerate}
  \item We break the data into K buckets.
  \item We train our model K times, each time using a different bucket as our testing set (and the remaining points as our training set).
  \item We average the results from the K training to get a final model.
\end{enumerate}

\subsection{Learning Curves}
If we plot on the x axis the number of training points used and on the y axis error, we can plot 2 series:

\begin{itemize}
  \item Error on training set
  \item Error on testing set
\end{itemize}

Those two curves behave differently depending on the goodness of the model:

\begin{itemize}
  \item High bias model (underfitting): the training and the testing errors converge to a medium to high error.
  \item Good model: the training and testing errors converge to a low error.
  \item High variance model (overfitting): the training and testing errors don’t converge. This is because overfitted models always have very low errors on the training set, but don’t generalize well, therefore having a large error on the testing set.
\end{itemize}



\end{document}
